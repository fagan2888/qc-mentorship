{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Variational Hamiltonian Ansatz on the Hubbard model\n",
    "\n",
    "The Hubbard model is a simplification of correlated electrons. Despite its simplistic structure, the Hubbard model is able to capture some interesting physics, such as the transition of a solid from a conducting to insulating state and some superconducting effects (though I don't yet understand any of this). \n",
    "\n",
    "In this notebook, we'll briefly describe the Hubbard model and then try to use the VHA to solve it. \n",
    "\n",
    "$\\newcommand{\\ket}[1]{\\lvert #1 \\rangle}$\n",
    "$\\newcommand{\\bra}[1]{\\langle #1 \\rvert}$\n",
    "$\\newcommand{\\braket}[1]{\\langle #1 \\rangle}$\n",
    "\n",
    "**Update 1**: I started by defining my own classes/functions for creation/annihilation operators, the Jordan-Wigner transform, the square lattice for the Hubbard Hamiltonian, the Hubbard Hamiltonian's matrix representation, and the Variational Hamiltonian Ansatz (VHA). As expected, my code and computer's RAM wasn't enough to work with anything more than a 2x2 square lattice (which has 8 qubits, so exists 256-dimensional Hilbert space). What did surprise me was how much more efficient OpenFermion was, because when I tried it when SSHing onto a Google Cloud computer, I was able to quickly solve the ground state of up to a 2x10 square lattice Hubbard Hamiltonian. Most of the functions/classes I defined are still available in the tools/ folder if you want to take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import scipy.optimize \n",
    "\n",
    "from tools.utils import * \n",
    "tol = 0.005 # Tolerance for elementwise equality of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hubbard-ham\"></a>\n",
    "### Defining the Hubbard Hamiltonian \n",
    "\n",
    "The Hubbard Hamiltonian: \n",
    "$$ H = -t \\sum_{\\braket{j, k}, \\sigma} \\Big( c^\\dagger_{j\\sigma} c_{k\\sigma} + c^\\dagger_{k \\sigma} c_{j \\sigma} \\Big) + U \\sum_j n_{j \\uparrow} n_{j \\downarrow} - \\mu \\sum_j \\Big(n_{j\\uparrow} + n_{j\\downarrow} \\Big) $$\n",
    "\n",
    "The first term is kinetic energy, a fermion moving from one site to another. The symbol $\\braket{j, k}$ implies iterating over sites that are adjacent. \n",
    "\n",
    "The second term is interaction energy, additional energy for a doubly-occupied site. \n",
    "\n",
    "The third term is chemical potential, which controls the filling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Hubbard Hamiltonian on a Square Lattice \n",
    "\n",
    "We'll use OpenFermion's `HubbardSquareLattice` class to define our lattice. OpenFermion has a simpler function `fermi_hubbard()` to create a `FermionOperator` to describe our system, but this doesn't give us access to the specific hopping terms in the Hamiltonian (the terms in the first summation above), which we'll need for the Variational Hamiltonian Ansatz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfermion.utils import HubbardSquareLattice\n",
    "# HubbardSquareLattice parameters\n",
    "x_n = 2\n",
    "y_n = 2\n",
    "n_dofs = 1 # 1 degree of freedom for spin, this might be wrong. Having only one dof means ordered=False. \n",
    "periodic = 0 # Not sure what this is, periodic boundary conditions?\n",
    "spinless = 0 # Has spin\n",
    "\n",
    "lattice = HubbardSquareLattice(x_n, y_n, n_dofs=n_dofs, periodic=periodic, spinless=spinless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a `FermiHubbardModel` instance by passing it our `HubbardSquareLattice` instance defined above. \n",
    "\n",
    "To get the `FermionOperator` instance, we need to call `FermiHubbardModel.hamiltonian()`. The [documentation](https://openfermion.readthedocs.io/en/latest/openfermion.html#openfermion.hamiltonians.FermiHubbardModel) isn't that great here, but the [source code](https://github.com/quantumlib/OpenFermion/blob/master/src/openfermion/hamiltonians/_general_hubbard.py) indicates we do indeed get a `FermionOperator` instance which we'll need for calculating ground state, etc. \n",
    "\n",
    "We can't just pass an integer for $t$, $U$, or $\\mu$ in this class. Instead, we have to specify the *specific* coefficient for each pair and edge type ($t_{ij}^{\\textrm{horizontal neighbor}}$). This will be useful later on, I think, because [1506.05135](https://arxiv.org/abs/1506.05135) says we'll need the indices for different values during adiabatic evolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfermion.hamiltonians import FermiHubbardModel\n",
    "from openfermion.utils import SpinPairs\n",
    "tunneling = [('neighbor', (0, 0), 1.)] # Not sure if this is right\n",
    "interaction = [('onsite', (0, 0), 2., SpinPairs.DIFF)] # Not sure if this is right\n",
    "# potential = [(0, 1.)]\n",
    "potential = None\n",
    "mag_field = 0. \n",
    "particle_hole_sym = False # Not sure if this is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubbard = FermiHubbardModel(lattice , tunneling_parameters=tunneling, interaction_parameters=interaction, \n",
    "                            potential_parameters=potential, magnetic_field=mag_field, \n",
    "                            particle_hole_symmetry=particle_hole_sym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Hamiltonian Ansatz\n",
    "\n",
    "The VHA is an ansatz inspired by time-evolution of the system. It splits the Hamiltonian into sub-operators and then does time-evolution for those operators: \n",
    "$$\\large U(\\theta) = \\prod_{k=1}^n \\prod_{\\alpha=1}^N \\exp \\Big( i\\theta_{\\alpha, k} H_{\\alpha} \\Big) $$\n",
    "where $H_\\alpha$ are the sub-Hamiltonians and $\\theta$ are the parameters being optimized. \n",
    "\n",
    "**Question:** Why is this a good ansatz? It seems it only has access to states that are evolutions of the state we start with. Why is that a guarantee that it'll approximate the ground state? \n",
    "\n",
    "**Answer:** It's based on adiabatic evolution. I don't understand this but somehow if we start by evolving the ground state of a Hamiltonian (non-interacting part in this case) and then slowly start replacing it with another Hamiltonian (full Hubbard Hamiltonian), we get the corresponding ground state of the new Hamiltonian by magic. \n",
    "\n",
    "In [1811.04476](https://arxiv.org/pdf/1811.04476.pdf) they use $N=5$, splitting it as I did above: even and odd horizontal hopping terms, even and odd vertical horizontal terms, and the on-site interaction terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "Ground state energy:  -3.6272130052966536\n"
     ]
    }
   ],
   "source": [
    "# Compute ground state on GCloud computers\n",
    "\n",
    "from openfermion import get_sparse_operator, get_ground_state\n",
    "hub_sparse = get_sparse_operator(hubbard.hamiltonian())\n",
    "print(hub_sparse.shape)\n",
    "genergy, gstate = get_ground_state(hub_sparse)\n",
    "print(\"Ground state energy: \", genergy)\n",
    "\n",
    "# \"Ground state energy: -3.6272130052966762\" \n",
    "# genergy = -3.6272130052966762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose ground state of tunneling term \n",
    "\n",
    "The *correct* starting state in the tunneling term will already have large overlap with the ground state of the Hubbard Hamiltonian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector v_tun[:, 1] has ovelap 0.11208473285869708.\n",
      "Eigenvector v_tun[:, 20] has ovelap 0.168593086346919.\n",
      "Eigenvector v_tun[:, 33] has ovelap 0.12464685509540482.\n",
      "Eigenvector v_tun[:, 36] has ovelap 0.133189141105224.\n",
      "Eigenvector v_tun[:, 39] has ovelap 0.12330712526686508.\n"
     ]
    }
   ],
   "source": [
    "from openfermion.utils import inner_product \n",
    "from openfermion.transforms import get_sparse_operator\n",
    "\n",
    "def overlap(a, b):\n",
    "    \"\"\"Calculates the overlap between vectors a and b. This metric is also known as fidelity. \"\"\"\n",
    "    inner = inner_product(a, b)\n",
    "    return (np.conjugate(inner) * inner).real\n",
    "\n",
    "# Lowest eigenvalue/vector of Hubbard Hamiltonian\n",
    "# The column v_hub[:, i] is the eigenvector corresponding to the eigenvalue w_hub[i]\n",
    "hub_sparse = get_sparse_operator(hubbard.hamiltonian())\n",
    "w_hub, v_hub = scipy.sparse.linalg.eigsh(hub_sparse, k=1, which='SA') \n",
    "\n",
    "# From looking at the spectrum,  I noticed the ground state was degenerate. \n",
    "# There were 16 eigenvectors with the lowest eigenvalue. \n",
    "# Actually, the initial state might not even be a ground state. Let's try \n",
    "# as many of the eigenvectors as we can. \n",
    "tun_sparse = get_sparse_operator(hubbard.tunneling_terms())\n",
    "# k can be at most n-2 where n is the dimension of the matrix\n",
    "w_tun, v_tun = scipy.sparse.linalg.eigsh(tun_sparse, k=np.shape(tun_sparse)[0]-2, which='SA')\n",
    "\n",
    "# Let's check overlap of each of the ground states. Hopefully, there's an obvious winner. \n",
    "for i in range(len(w_tun)):\n",
    "    fid = overlap(v_hub[:, 0], v_tun[:, i]) \n",
    "    if fid > 0.10:\n",
    "        print('Eigenvector v_tun[:, {}] has ovelap {}.'.format(i, fid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest overlap is about 18%, but this doesn't look too promising. Here's a trick we can use: if we *perturb* the tunneling term with the interaction term and then get the ground state, we'll find it has maximum overlap with the Hubbard model as the perturbation goes to 0. Here's what that looks like in code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector v_per[:, 0] has overlap 0.9770315592604039.\n",
      "Eigenvector v_per[:, 1] has overlap 1.0578811998901987e-19.\n",
      "Eigenvector v_per[:, 2] has overlap 1.7289770398736907e-25.\n",
      "Eigenvector v_per[:, 3] has overlap 6.113531933914778e-26.\n",
      "Eigenvector v_per[:, 4] has overlap 5.598907976260925e-26.\n"
     ]
    }
   ],
   "source": [
    "s = 1e-4\n",
    "int_sparse = get_sparse_operator(hubbard.interaction_terms())\n",
    "perturbed_sparse = tun_sparse + s * int_sparse\n",
    "\n",
    "w_per,v_per = scipy.sparse.linalg.eigsh(perturbed_sparse, k=5, which='SA') \n",
    "\n",
    "for i in range(len(w_per)):\n",
    "    fid = overlap(v_hub[:, 0], v_per[:, i])\n",
    "    print('Eigenvector v_per[:, {}] has overlap {}.'.format(i, fid))\n",
    "    \n",
    "# WOW, there's a clear winner here v_per[:, 0]\n",
    "per_state_most_overlap = v_per[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should use the eigenvector `w_per[:, 0]` because it had a 97% overlap with the Hubbard ground state. In reality, we wouldn't have been able to efficiently find this state because it's perturbed with the interaction term and therefore not quadratic. Instead, we'll find the tunneling eigenvector that's closest to `v_per[:, 0]` and use that instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector v_tun[:, 20] had the maximum overlap of 0.17255743072026664 with the best perturbed state.\n"
     ]
    }
   ],
   "source": [
    "max_overlap = 0\n",
    "index_max_overlap = 0\n",
    "\n",
    "for i in range(len(w_tun)):\n",
    "    fid = overlap(per_state_most_overlap, v_tun[:, i])\n",
    "    if fid > max_overlap: \n",
    "        max_overlap = fid \n",
    "        index_max_overlap = i \n",
    "print(\"Eigenvector v_tun[:, {}] had the maximum overlap of {} with the best perturbed state.\".format(\n",
    "    index_max_overlap, max_overlap))\n",
    "initial_state = v_tun[:, index_max_overlap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to specify the circuit for this starting state, so that we can start from the computational all-zero state $\\ket{0...0}$ and then apply a circuit to end up in `v_tun[:, 34]`. \n",
    "\n",
    "OpenFermion provides us with the function `prepare_gaussian_state(qubits, quadratic_hamiltonian, occupied_orbitals=None)` which returns the Circuit to prepare an eigenvector of a quadratic Hamiltonian. \n",
    "\n",
    "Of course, there could be many eigenvectors (our Hamiltonian matrix has 2<sup>8</sup> = 256 dimensions so 256 eigenvectors). We need some way of specifying which one we want. That's what the `occupied_orbitals` argument is for. \n",
    "\n",
    "[Read this issue for an explanation of how this works](https://github.com/quantumlib/OpenFermion/issues/284)\n",
    "\n",
    "Since we diagonalized our quadratic Hamiltonian, we can write it as $\\sum_k \\epsilon_k a_k^\\dagger a_k$, which indicates the eigenvalues will be subsets of $\\epsilon_k$. To specify a specific eigenvector, one way is to specify its eigenvalue. \n",
    "\n",
    "But this doesn't work because there might be more than eigenvector for a specific eigenvalue! We call these degenerate states. Suppose our $\\epsilon_k$ were [-2, 0, 0, -2]. Then, we have 4 different eigenvectors for the eigenvalue -4, because we can choose both -2's and then decide to pick some of the 0's to yield -4. There 4 ways to choose the 0's, so we have 4 eigenvectors for that same eigenvalue. \n",
    "\n",
    "In OpenFermion, we can get this array of $\\epsilon_k$ with `orbital_energies, constant = quadratic_hamiltonian.orbital_energies()`. Then we specify which of these we want by passing an array into `prepare_gaussian_state`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalue is -3.999999999999995\n"
     ]
    }
   ],
   "source": [
    "# We should find what eigenvalue our desired eigenvector \n",
    "# has so we don't have to search through all 256 eigenvectors \n",
    "\n",
    "# We used v_tun[:, 34] so our eigenvalue is w_tun[34]\n",
    "print(\"The eigenvalue is {}\".format(w_tun[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this was one of the ground states since -4 is the minimum eigenvalue. We just have to try all the `orbital_energies` subsets that have a sum of -4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.0, -2.0, -0.0, 0.0, 0.0, 0.0, 2.0, 2.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openfermion.transforms import get_quadratic_hamiltonian \n",
    "\n",
    "# Convert to QuadraticHamiltonian instance \n",
    "tun_quad = get_quadratic_hamiltonian(hubbard.tunneling_terms())\n",
    "\n",
    "orbital_energies, constant = tun_quad.orbital_energies()\n",
    "[round(e, 2) for e in orbital_energies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use both -2's, and then we can pick any combination of the four 0's. We'll try all of them and then calculate which yields the maximum overlap with our desired state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbital energies [0, 1] resulted in a state with 0.17255741661035914 overlap with our desired state.\n"
     ]
    }
   ],
   "source": [
    "# Get all possible combinations of orbital energies which have a sum of -4 \n",
    "import itertools \n",
    "zero_indices = [2, 3, 4, 5] # Indices of 0's in orbital_energies\n",
    "orbital_energies_combs = [] \n",
    "for r in range(len(zero_indices) + 1):\n",
    "    for subset in itertools.combinations(zero_indices, r):\n",
    "        # Include indices for both -2's and then indices of some 0's\n",
    "        orbital_energies_combs.append([0, 1] + list(subset))\n",
    "                \n",
    "# Now, generate each eigenvector from a combination of orbital_energies \n",
    "# and compare it to our desired tunneling eigenvector. \n",
    "# This way we know which orbital_energies combination yields our \n",
    "# desired eigenvector. \n",
    "\n",
    "# from openfermion.utils import gaussian_state_preparation_circuit\n",
    "from openfermioncirq import prepare_gaussian_state\n",
    "from cirq import Circuit, final_wavefunction, LineQubit\n",
    "\n",
    "overlaps = []\n",
    "for comb in orbital_energies_combs:\n",
    "    state = final_wavefunction(Circuit(prepare_gaussian_state(\n",
    "            LineQubit.range(8), # We have an 8 qubit Hamiltonian \n",
    "            tun_quad, \n",
    "            occupied_orbitals=comb)))\n",
    "    overlaps.append(overlap(state, initial_state))\n",
    "\n",
    "# Find state with top overlap and save the Circuit object that created it \n",
    "best_state_index = np.argmax(overlaps)\n",
    "prep_circ = Circuit(prepare_gaussian_state(\n",
    "    LineQubit.range(8), tun_quad, occupied_orbitals=orbital_energies_combs[best_state_index]))\n",
    "\n",
    "print(\"Orbital energies {} resulted in a state with {} overlap with our desired state.\".format(\n",
    "    orbital_energies_combs[best_state_index], overlaps[best_state_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY IS THIS OVERLAP SO LOW??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running VQE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfermioncirq import SwapNetworkTrotterHubbardAnsatz\n",
    "\n",
    "steps = 5 \n",
    "ansatz = SwapNetworkTrotterHubbardAnsatz(x_n, y_n, 1., 2., periodic=False, iterations=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal ground state energy is -3.499999999999987\n"
     ]
    }
   ],
   "source": [
    "from openfermioncirq import HamiltonianObjective, VariationalStudy \n",
    "from openfermioncirq.optimization import ScipyOptimizationAlgorithm, OptimizationParams\n",
    "\n",
    "# Define our objective function as the expectation of the Hubbard Hamiltonian\n",
    "obj = HamiltonianObjective(hubbard.hamiltonian())\n",
    "\n",
    "# Define our VariationalStudy \n",
    "study = VariationalStudy(\n",
    "    'Hubbard-VHA', \n",
    "    ansatz, \n",
    "    obj, \n",
    "    preparation_circuit=prep_circ)\n",
    "\n",
    "# Choose our optimization algorithm \n",
    "algorithm = ScipyOptimizationAlgorithm(kwargs={'method': 'L-BFGS-B'}, uses_bounds=True)\n",
    "optimization_params = OptimizationParams(algorithm=algorithm)\n",
    "\n",
    "# Optimize \n",
    "result = study.optimize(optimization_params)\n",
    "\n",
    "print(\"Optimal ground state energy is {}\".format(result.optimal_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.6272130052966736"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hub[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much overlap the ground state VHA outputted has with the true ground state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VHA ground state and true ground state have an overlap of 0.977026508501244\n"
     ]
    }
   ],
   "source": [
    "from cirq import resolve_parameters\n",
    "\n",
    "optimal_params = study.trial_results[0].optimal_parameters\n",
    "VHA_ground_state = final_wavefunction(resolve_parameters(\n",
    "    study.circuit, study.ansatz.param_resolver(optimal_params)))\n",
    "\n",
    "print(\"VHA ground state and true ground state have an overlap of {}\".format(\n",
    "    overlap(v_hub[:, 0], VHA_ground_state)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing optimal parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06366198, -0.06366198, -0.0063662 , -0.06366198, -0.06366198,\n",
       "       -0.01909859, -0.06366198, -0.06366198, -0.03183099, -0.06366198,\n",
       "       -0.06366198, -0.04456338, -0.06366198, -0.06366198, -0.05729578])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 15 parameters - 3 for each of the 5 steps. Notice the first 2 of each step (the tunneling term parameters) are the same, like we'd expect for adiabatic evolution. The final parameter in each step seems to increment by a constant amount. Looks like we exactly followed adiabatic evolution..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing ground state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see how many non-zero elements the state vector of the ground state has: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 256-dimensionsal ground state vector has 16 non-zero elements.\n"
     ]
    }
   ],
   "source": [
    "total_elems = 0\n",
    "for elem in VHA_ground_state: \n",
    "    if np.abs(elem) > tol: \n",
    "        total_elems += 1\n",
    "print(\"The {}-dimensionsal ground state vector has {} non-zero elements.\".format(\n",
    "    VHA_ground_state.shape[0], total_elems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It's very sparse. If there was only 1 non-zero element, then we could decompose it into tensor products in the computational basis state easily. Let's see what the individual elements are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm is 1.000000238418579\n",
      "The distinct elements in the ground state vector along with their counts are: \n",
      " {'0j': 240, '(0.10403669-0.22732446j)': 10, '(-0.10403668+0.22732443j)': 6}\n"
     ]
    }
   ],
   "source": [
    "# Make sure norm is 1 \n",
    "print(\"Norm is {}\".format(np.linalg.norm(VHA_ground_state)))\n",
    "             \n",
    "# This finds every distinct element in VHA_ground_state (up to a tolerance) \n",
    "# and counts its occurences\n",
    "common_elems = {}\n",
    "for elem in VHA_ground_state: \n",
    "    # Check if it's already in common_elems \n",
    "    array_common_elems = [complex(e) for e in list(common_elems.keys())]\n",
    "    match = np.isclose(array_common_elems, elem, rtol=0, atol=tol)\n",
    "    if match.any():\n",
    "        common_elems[str(array_common_elems[match.argmax()])] += 1\n",
    "    else: \n",
    "        common_elems[str(elem)] = 1\n",
    "        \n",
    "print(\"The distinct elements in the ground state vector along with their counts are: \\n\", common_elems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, 10 of the non-zero elements have the same value, let's call this $a$. And the other 6 have the value $-a$. I guess I could decompose this into a sum of 16 pure states, but I don't think we'll get much insight into the system with that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just get the average measurement for each of the qubits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirq import measure_state_vector\n",
    "\n",
    "hundred_thousand_measurements = [measure_state_vector(VHA_ground_state, range(8))[0] for _ in range(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24854, 0.25112, 0.25269, 0.24896, 0.24897, 0.24906, 0.2498 ,\n",
       "       0.25086])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(hundred_thousand_measurements, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's boring! Each qubit has an average value of 0.25. They're completely uniform! \n",
    "\n",
    "Let's think about what this means. We used Jordan-Wigner encoding which means the qubit corresponding to a spin-orbital is $\\ket{1}$ if there's an electron in that spin-orbital, and $\\ket{0}$ if there isn't. The uniform is indeed half-filling. \n",
    "\n",
    "This also means we don't prefer any spin over the other, which makes it unlikely magnetic moments will form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking number of trials with two 1's\n",
    "for trial in hundred_thousand_measurements: \n",
    "    if np.count_nonzero(trial) != 2: \n",
    "        print(np.count_nonzero(trial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a single measurement where we didn't get two 1's. This means we were wrong! The ground state distribution of electrons *isn't* random - they're entangled in some way to give us at most 2 electrons in the 4 site model. \n",
    "\n",
    "Well, this is a little more interesting. I wonder if the spin-up and spin-down results are correlated in any way. Here's how we'll check this: I suspect (but I need to find out for sure) that the first 4 qubits correspond to spin-UP and the last 4 qubits correspond to spin-DOWN, so that indices $i$ and $i+4$ correspond to the spin-UP and spin-DOWN electrons on the $i$th site. This means if there's always a 1 in the first 4 qubits and a 1 in the last 4 qubits, there's always one spin-UP and one spin-DOWN electron. We can also check how often they're on the same site, though I suspect this will never be the case because of the interaction energy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials with 2 spin-UPs or 2 spin-DOWNs: 50101\n",
      "Number of trials with 2 electrons on the same site: 0\n"
     ]
    }
   ],
   "source": [
    "total_unequal_spins = 0\n",
    "total_correlated_spins = 0\n",
    "for trial in hundred_thousand_measurements:\n",
    "    if np.count_nonzero(trial[:4]) != 1: # If 1 here, then 1 on the last 4 too \n",
    "        total_unequal_spins += 1\n",
    "                \n",
    "    if np.argmax(trial[:4]) == np.argmax(trial[4:]) - 4: \n",
    "        total_correlated_spins += 1\n",
    "\n",
    "print(\"Number of trials with 2 spin-UPs or 2 spin-DOWNs: {}\".format(total_unequal_spins))\n",
    "print(\"Number of trials with 2 electrons on the same site: {}\".format(total_correlated_spins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh. Looks like my guess was wrong about the first one. In about half of the trials, there were 2 spin-UPS or 2 spin-DOWNs. This means there's a chance for magnetism in these ones. On average though, if the number of trials with 2 spin-UPs is the same as the number of trials with 2 spin-DOWNS, then there's no magnetism *on average*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials with 2 spin-UPs: 25116\n",
      "Number of trials with 2 spin-DOWNs: 24985\n"
     ]
    }
   ],
   "source": [
    "total_2_spin_UP = 0 \n",
    "total_2_spin_DOWN = 0\n",
    "\n",
    "for trial in hundred_thousand_measurements:\n",
    "    if np.count_nonzero(trial[:4]) == 2: \n",
    "        total_2_spin_UP += 1\n",
    "    elif np.count_nonzero(trial[4:]) == 2: \n",
    "        total_2_spin_DOWN += 1\n",
    "print(\"Number of trials with 2 spin-UPs: {}\".format(total_2_spin_UP))\n",
    "print(\"Number of trials with 2 spin-DOWNs: {}\".format(total_2_spin_DOWN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, they're about the same, so on average, there's no magnetism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing VQE\n",
    "\n",
    "I end up needing to rewrite most of the `SwapNetworkTrotterAnsatz` from OpenFermion to get more control over our initialization strategy. [1811.04476](https://arxiv.org/abs/1811.04476) says initial parameters turned out to be very important for reaching the ground state, and they were able to get 100% overlap on the 2x2 Hubbard model, so I'll try their strategies. \n",
    "\n",
    "Refer to the docstrings for more info on implementation, but broadly, the initialization strategies are: \n",
    "- Adiabatically turn on the interacting term \n",
    "- Adiabatically turn on the tunneling and interaction terms \n",
    "- Trotter expansion where all parameters are the same \n",
    "- Adiabatically turn on interaction for shorter time. \n",
    "\n",
    "We also try to implement the \"full optimization\" method outlined in [1506.05135](https://arxiv.org/abs/1506.05135). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple callback function we can pass into the optimizer to tell us what iteration it's on\n",
    "# Nevermind: this doesn't work with study.save()\n",
    "def gen_callback():\n",
    "    i = 0 \n",
    "    def callback(xk):\n",
    "        nonlocal i\n",
    "        print(\"Iteration: \", i)\n",
    "        i += 1\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-2da4646f5826>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-2da4646f5826>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    iterations: int=1,from\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class CustomHubbard(SwapNetworkTrotterHubbardAnsatz):\n",
    "    # Redefining my own Hubbard ansatz to override default_initial_params(self) \n",
    "    def __init__(self,\n",
    "                 x_dim: float,\n",
    "                 y_dim: float,\n",
    "                 tunneling: float,\n",
    "                 coulomb: float,\n",
    "                 initial_param_method: str, # This is the only change! \n",
    "                 periodic: bool=True,\n",
    "                 iterations: int=1,from \n",
    "                 adiabatic_evolution_time=None,\n",
    "                 qubits=None\n",
    "                 ) -> None:\n",
    "        \n",
    "        possible_initial_param_methods = [\n",
    "            'adiabatic', 'adiabatic_tunneling', 'trotter_constant', 'adiabatic_short'\n",
    "        ]\n",
    "        if initial_param_method not in possible_initial_param_methods:\n",
    "            raise ValueError('initial_param_method must be one of {}'.format(possible_initial_param_methods))\n",
    "        self.initial_param_method = initial_param_method\n",
    "        \n",
    "        super().__init__(x_dim, y_dim, tunneling, coulomb, periodic=periodic, iterations=iterations, \n",
    "                         adiabatic_evolution_time=adiabatic_evolution_time, qubits=qubits)\n",
    "        \n",
    "    def default_initial_params(self):\n",
    "        total_time = self.adiabatic_evolution_time\n",
    "        step_time = total_time / self.iterations \n",
    "        \n",
    "        params = [] \n",
    "        # I'm going to keep the format of the original default_initial_params (the -self.tunneling and \n",
    "        # division by pi) even though I don't understand why they do this yet. \n",
    "        if self.initial_param_method == 'adiabatic': \n",
    "            \"\"\"\n",
    "            Approximate evolution by H(t) = T + (t/A)V.\n",
    "            Sets the parameters so that the ansatz circuit consists of a sequence\n",
    "            of second-order Trotter steps approximating the dynamics of the\n",
    "            time-dependent Hamiltonian H(t) = T + (t/A)V, where T is the one-body\n",
    "            term and V is the two-body term of the Hamiltonian used to generate the\n",
    "            ansatz circuit, and t ranges from 0 to A, where A is equal to\n",
    "            `self.adibatic_evolution_time`. The number of Trotter steps\n",
    "            is equal to the number of iterations in the ansatz. This choice is\n",
    "            motivated by the idea of state preparation via adiabatic evolution.\n",
    "            The dynamics of H(t) are approximated as follows. First, the total\n",
    "            evolution time of A is split into segments of length A / r, where r\n",
    "            is the number of Trotter steps. Then, each Trotter step simulates H(t)\n",
    "            for a time length of A / r, where t is the midpoint of the\n",
    "            corresponding time segment. As an example, suppose A is 100 and the\n",
    "            ansatz has two iterations. Then the approximation is achieved with two\n",
    "            Trotter steps. The first Trotter step simulates H(25) for a time length\n",
    "            of 50, and the second Trotter step simulates H(75) for a time length\n",
    "            of 50.\n",
    "\n",
    "            The above docstring was copied from OpenFermion. \n",
    "            \"\"\"\n",
    "            for param, scale_factor in zip(self.params(),\n",
    "                                           self.param_scale_factors()):\n",
    "                if param.letter == 'Th' or param.letter == 'Tv':\n",
    "                    params.append(_canonicalize_exponent(\n",
    "                        -self.tunneling * step_time / np.pi, 4) / scale_factor)\n",
    "                elif param.letter == 'V':\n",
    "                    i, = param.subscripts\n",
    "                    # Use the midpoint of the time segment\n",
    "                    interpolation_progress = 0.5 * (2 * i + 1) / self.iterations\n",
    "                    params.append(_canonicalize_exponent(\n",
    "                        -0.5 * self.coulomb * interpolation_progress *\n",
    "                        step_time / np.pi, 2) / scale_factor)\n",
    "\n",
    "        elif self.initial_param_method == 'adiabatic_tunneling': \n",
    "            \"\"\"\n",
    "            Instead of only adiabatically turning on the interaction term, we also \n",
    "            adiabatically turn on the tunneling term. \n",
    "            \n",
    "            Inspired by 1811.04476. \n",
    "            \"\"\"\n",
    "            for param, scale_factor in zip(self.params(),\n",
    "                                           self.param_scale_factors()):\n",
    "                i, = param.subscripts\n",
    "                # Use the midpoint of the time segment\n",
    "                interpolation_progress = 0.5 * (2 * i + 1) / self.iterations\n",
    "\n",
    "                if param.letter == 'Th' or param.letter == 'Tv':\n",
    "                    params.append(_canonicalize_exponent(\n",
    "                        -self.tunneling * interpolation_progress * \n",
    "                        step_time / np.pi, 4) / scale_factor)\n",
    "                elif param.letter == 'V':\n",
    "                    params.append(_canonicalize_exponent(\n",
    "                        -0.5 * self.coulomb * interpolation_progress *\n",
    "                        step_time / np.pi, 2) / scale_factor)\n",
    "        \n",
    "        elif self.initial_param_method == 'trotter_constant': \n",
    "            \"\"\"\n",
    "            This is just Trotter decomposition for a simulation of length total_time. \n",
    "            I don't use _canonicalize_exponent(), so check what changes and if anything breaks. \n",
    "            \n",
    "            Inspired by 1811.04476.\n",
    "            \"\"\"\n",
    "            for param, scale_factor in zip(self.params(),\n",
    "                                           self.param_scale_factors()):\n",
    "                params.append(step_time)\n",
    "\n",
    "        elif self.initial_param_method == 'adiabatic_short':\n",
    "            \"\"\"\n",
    "            OpenFermion sets adiabatic_evolution_time = 0.1*abs(coulomb)*iterations, if we don't \n",
    "            provide one ourselves. This initialization method will divide that by iterations. \n",
    "            \n",
    "            Inspired by 1811.04476. \n",
    "            \"\"\"\n",
    "            total_time /= 5\n",
    "            step_time /= 5\n",
    "            for param, scale_factor in zip(self.params(),  self.param_scale_factors()):\n",
    "                # Copied 'adiabatic'\n",
    "                if param.letter == 'Th' or param.letter == 'Tv':\n",
    "                    params.append(_canonicalize_exponent(\n",
    "                        -self.tunneling * step_time / np.pi, 4) / scale_factor)\n",
    "                elif param.letter == 'V':\n",
    "                    i, = param.subscripts\n",
    "                    # Use the midpoint of the time segment\n",
    "                    interpolation_progress = 0.5 * (2 * i + 1) / self.iterations\n",
    "                    params.append(_canonicalize_exponent(\n",
    "                        -0.5 * self.coulomb * interpolation_progress *\n",
    "                        step_time / np.pi, 2) / scale_factor)\n",
    "                    \n",
    "        else: \n",
    "            raise ValueError(\"Don't know how to interpret initial parameter method {}\".format(self.initial_param_method))\n",
    "\n",
    "        return np.array(params)\n",
    "    \n",
    "def _canonicalize_exponent(exponent: float, period: int) -> float:\n",
    "    # Shift exponent into [-p/2, +p/2).\n",
    "    # They chose period = bounds (4 for T, 2 for V)\n",
    "    exponent += period / 2\n",
    "    exponent %= period\n",
    "    exponent -= period / 2\n",
    "    # Prefer (-p/2, +p/2] over [-p/2, +p/2).\n",
    "    if exponent <= -period / 2:\n",
    "        exponent += period  # coverage: ignore\n",
    "    return exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfermioncirq import HamiltonianObjective, VariationalStudy\n",
    "from openfermioncirq.optimization import ScipyOptimizationAlgorithm, OptimizationParams\n",
    "from datetime import datetime\n",
    "\n",
    "# Optimizes and saves our VariationalStudy\n",
    "def run_ansatz(index_orbital_energies_combs, initialization_strat):\n",
    "    steps = 10\n",
    "    \n",
    "    obj = HamiltonianObjective(dc_hub) # Define objective function as Hamiltonian averaging\n",
    "    \n",
    "#     ansatz = SwapNetworkTrotterHubbardAnsatz(x_n, y_n, 1., 2., periodic=False, iterations=steps)\n",
    "    ansatz = CustomHubbard(x_n, y_n, 1., 2., initialization_strat, periodic=False, iterations=steps, \n",
    "                           adiabatic_evolution_time=50.)\n",
    "    \n",
    "    prep_circ = Circuit(\n",
    "        prepare_gaussian_state(\n",
    "            ansatz.qubits, \n",
    "            QuadraticHamiltonian(dc_hub.one_body), \n",
    "            occupied_orbitals=orbital_energies_combs[index_orbital_energies_combs]\n",
    "        ))\n",
    "    time = datetime.now().strftime(\"%m.%d.%y-%H:%M:%S\")\n",
    "    study = VariationalStudy(\n",
    "        'Hubbard-VHA-{}-{}-{}'.format(index_orbital_energies_combs, initialization_strat, time), \n",
    "        ansatz, \n",
    "        obj, \n",
    "        preparation_circuit=prep_circ, \n",
    "        target=genergy, \n",
    "        datadir='SavedVariationalStudy'\n",
    "    )\n",
    "    \n",
    "    algorithm = ScipyOptimizationAlgorithm(\n",
    "        kwargs={'method': 'L-BFGS-B', \n",
    "#                 'tol': 1e-16\n",
    "               }, \n",
    "#         options={'maxiter': 100}, \n",
    "        uses_bounds=True)\n",
    "    \n",
    "    optimizaton_params = OptimizationParams(algorithm=algorithm)\n",
    "    \n",
    "    # Optimize\n",
    "    result = study.optimize(optimizaton_params)\n",
    "    \n",
    "    study.save()\n",
    "    return result.optimal_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top_three:\n",
    "    for init_strat in ['adiabatic', 'adiabatic_tunneling', 'trotter_constant', 'adiabatic_short']:\n",
    "        print(\"For tunneling ground state with index {} and initialization strategy {},the Hubbard ground state energy is {}\".format(\n",
    "            i, init_strat, run_ansatz(i, init_strat)))\n",
    "\n",
    "# We don't see that much improvement..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "path = 'SavedVariationalStudy/' + os.listdir('SavedVariationalStudy')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# So once we do that optimization, we have decent overlap and we have our parameters. \n",
    "\"\"\"\n",
    "It seems they do an iterated VQE: do VQE for H(1/s) using ground state of H(0), then do VQE for H(2/s) using \n",
    "ground state of H(1/s), ..., until they solve VQE for H(1). They call this the result of \"sequential optimization\"\n",
    "and it isn't *that* close to the ground state (~70% overlap for 3 iterations with 2x2). \n",
    "Then, they used these resulting parameters as the initial parameters for an optimization method they call the \n",
    "\"global vairational\" method, calling this final result \"full optimziation\". \n",
    "Here's what the global variational method does: \n",
    "1. Choose 6 random points near the initial parameters\n",
    "2. For each point, do greedy noisy search for 150 iterations: \n",
    "    a. slightly perturb point\n",
    "        i. If number of acceptances in last 30 trials was large, increase step size. Else, decrease it. \n",
    "    b. keep the new value if it reduces energy\n",
    "3. Do Powell on each point until convergence. \n",
    "4. Keep the point with lowest energy. \n",
    "5. Alternate greedy noisy search and Powell until neither finds improvement. \n",
    "\"\"\"\n",
    "\n",
    "# study = VariationalStudy.load('SavedVariationalStudy/Hubbard-VHA-0-adiabatic-5.13.20-09:39:03')\n",
    "study = VariationalStudy.load(path)\n",
    "params = study.trial_results[0].optimal_parameters \n",
    "\n",
    "def init_6(params):\n",
    "    \"\"\"Initialize 6 'points' that we optimize. Step 1 above. \"\"\"\n",
    "    new_points = [] \n",
    "    for i in range(6):\n",
    "        gaussian_noise = np.random.normal(size=params.size) \n",
    "        new_points.append(params + gaussian_noise)\n",
    "    return new_points\n",
    "\n",
    "def greedy_noisy_search(point, iterations=150):\n",
    "    \"\"\"Do greedy noisy search as described above. \"\"\"\n",
    "    min_point = point\n",
    "    values = []\n",
    "    # Standard deviation of noise - this might not work \n",
    "    step_size = 0.5\n",
    "    for i in range(iterations):\n",
    "        gaussian_noise = np.random.normal(scale=step_size, size=point.size)\n",
    "        # Get value of new parameters\n",
    "        new_val = study.value_of(point + gaussian_noise)\n",
    "        values.append(new_val) \n",
    "        if new_val == min(values): \n",
    "            step_size /= 0.8 \n",
    "            min_point = new_val\n",
    "            print('Minimum is now {}'.format(min_point))\n",
    "        elif np.random.randint(20) == 0: # Do this 1/20 times\n",
    "            step_size *= 0.8\n",
    "        print(i, step_size)\n",
    "    return min_point\n",
    "\n",
    "def powell(point):\n",
    "    res = scipy.optimize.minimize(study.value_of, point, method='Powell', tol=1e-10)\n",
    "    return res.x.real\n",
    "\n",
    "def full_optimization(params):\n",
    "    six_starter_points = init_6(params)\n",
    "    for i in range(len(six_starter_points)):\n",
    "        six_starter_points[i] = greedy_noisy_search(six_starter_points[i])\n",
    "#         six_starter_points[i] = powell(six_starter_points[i])\n",
    "        print(six_starter_points[i])\n",
    "    vals = [study.value_of(point) for point in six_starter_points]\n",
    "    \n",
    "    try:\n",
    "        best = six_starter_points[np.argmin(vals)]\n",
    "        best_val = study.value_of(best)\n",
    "        curr = 'greedy'\n",
    "        while True: \n",
    "            old_best = best\n",
    "            if curr == 'greedy':\n",
    "                best = greedy_noisy_search(best)\n",
    "                curr = 'powell'\n",
    "            else: \n",
    "                best = powell(best)\n",
    "                curr = 'greedy'\n",
    "            if np.abs(study.value_of(best) - study.value_of(old_best)) < 1e-10:\n",
    "                break \n",
    "    except:\n",
    "        return best\n",
    "    return best\n",
    "            \n",
    "            \n",
    "full_optimization(params)\n",
    "# Errors when I run Powell... but either way not seeing much improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we have an error of about 0.12. I suspect this might be the Trotter error, which means Jan-Michael's \n",
    "# decomposition will fix it. If it does, then I am justified in working on measurement precision. \n",
    "# For now, let's check the overlap with our true ground state\n",
    "\n",
    "from cirq import resolve_parameters\n",
    "from openfermioncirq.variational import variational_black_box\n",
    "\n",
    "def get_optimal_ground_state(study): \n",
    "    # Modified variational_black_box.UNITARY_SIMULATE.evaluate_noiseless \n",
    "    black_box = study._black_box_type(\n",
    "        study.ansatz, \n",
    "        study.objective, \n",
    "        study._preparation_circuit, \n",
    "        study.initial_state)\n",
    "    \n",
    "    circuit = resolve_parameters(\n",
    "        black_box.preparation_circuit + black_box.ansatz.circuit, \n",
    "        black_box.ansatz.param_resolver(black_box.ansatz.default_initial_params()))\n",
    "    final_state = circuit.final_wavefunction(\n",
    "        black_box.initial_state, \n",
    "        qubit_order=black_box.ansatz.qubit_permutation(black_box.ansatz.qubits))\n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD STUDY WITH BEST VALUE AND CHECK FIDELITY\n",
    "study = VariationalStudy.load(path)\n",
    "\n",
    "opt_ground = get_optimal_ground_state(study)\n",
    "fidelity(opt_ground, v_hub[:,0]).real\n",
    "# 97.7% fidelity is great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we implement the above exponentials in a quantum circuit? Hm... \n",
    "\n",
    "OF has a `SwapNetworkTrotterHubbard` ansatz. How does it work? \n",
    "\n",
    "Well, the SwapNetwork uses only `ISWAP`, `PhasedISWAP`, `CZ` and `Z` gates. What does it do? \n",
    "\n",
    "It was proposed in arxiv: 1711.04789. It allows us to simulate a Trotter step in $N$ depth and $N^2 / 2$ two-qubit entangling gates, and lets us prepare arbitrary Slater determinants in $N/2$ depth, all assuming only a linearly connected architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Btw, this our ansatz\n",
    "print('Created a variational study with {} qubits and {} parameters'.format(\n",
    "    len(study.ansatz.qubits), study.num_params))\n",
    "\n",
    "print(\"The value of the objective with default initial parameters is {}\".format(\n",
    "    study.value_of(ansatz.default_initial_params())))\n",
    "\n",
    "print(\"The circuit of the study is:\")\n",
    "print(study.circuit.to_text_diagram(transpose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the circuit is made up of only 2 main gates: `ISWAPPowGate` and `CZPowGate`. The Cirq documentation tells us that the matrix decomposition for these gates is: \n",
    "$$\\texttt{ISWAPPowGate}(t) = \\begin{bmatrix} \n",
    "1 & 0 & 0 & 0 \\\\ \n",
    "0 & \\cos(\\frac{\\pi t}{2}) & i \\sin \\frac{\\pi t}{2} & 0 \\\\\n",
    "0 & i \\sin(\\frac{\\pi t}{2}) & \\cos(\\frac{\\pi t}{2}) & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix} \\qquad \\texttt{CZPowGate}(t) = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & \\exp(i \\pi t) \n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We have 60 parameters. What are they? \n",
    "# They're in format (U/T/W/V, p, q, i) where p,q are qubits and i is iteration\n",
    "# Seems 12 parameters per iteration. Per iteration, 4 interaction terms like we'd expect, \n",
    "# and then 8 tunneling terms like we'd expect. \n",
    "\n",
    "# How can I make these fewer? YO, SwapNetworkTrotterHubbard has only 3 parameters per iteration!\n",
    "list(ansatz.params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this mean?\n",
    "obj.variance_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking commutator relations of JW for Trotterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfermion.utils import commutator \n",
    "from openfermion.ops import FermionOperator \n",
    "from openfermion.transforms import jordan_wigner \n",
    "\n",
    "tunneling_01 = FermionOperator('0^ 1') + FermionOperator('1^ 0')\n",
    "tunneling_12 = FermionOperator('1^ 2') + FermionOperator('2^ 1')\n",
    "tunneling_23 = FermionOperator('2^ 3') + FermionOperator('3^ 2')\n",
    "\n",
    "print('Commutator of 01 and 12 is: ', commutator(tunneling_01, tunneling_12))\n",
    "print('\\nJW matrix of above commutator is: ', commutator(jordan_wigner(tunneling_01), jordan_wigner(tunneling_12)))\n",
    "print('\\nCommutator of 01 and 23 is: ', commutator(tunneling_01, tunneling_23))\n",
    "print('\\nJW matrix of above commutator is: ', commutator(jordan_wigner(tunneling_01), jordan_wigner(tunneling_23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jordan_wigner(FermionOperator('1^')) * jordan_wigner(FermionOperator('0^'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.25 * NKron(X, X) + 0.25 * NKron(X, Y) + 0.25 * NKron(Y, X) + 0.25 * NKron(Y, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_tun = FermionOperator('0^ 1') + FermionOperator('1^ 0') + FermionOperator('2^ 3') + FermionOperator('3^ 2')\n",
    "ver_tun = FermionOperator('0^ 2') + FermionOperator('2^ 0') + FermionOperator('1^ 3') + FermionOperator('3^ 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commutator(jordan_wigner(hor_tun), jordan_wigner(ver_tun))\n",
    "# Wait so horizontal and vertical terms commute... and the individual horizontal and vertical terms commute for \n",
    "# 2x2 case. Then I should have no error at all! It's just that I need more steps... But other papers didn't need \n",
    "# more steps. I legit just have to optimize longer then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to reduce Trotter error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HubbardSquareLattice` class has a useful method: `site_pair_iter(edge_type)`. We'll use the 'horizontal_neighbor' and 'vertical_neighbor' edge types. But we also need to differentiate between \"even\" and \"odd\" horizontal and vertical neighbors: even horizontal neighbors will be horizontal neighbors whose leftmost site has even index, and likewise for horizontal odd, vertical even, and vertical odd neighbors. To get this added specificity, we'll need to examine each item in the iterable generated by `site_pair_iter()`. We need the even and odd terms because then when we Trotterize the Hubbard Hamiltonian, we don't introduce any Trotter error, because the our four sets of tunneling terms (even horizontal, even vertical, odd horizontal, odd vertical) commute with each other. \n",
    "\n",
    "Actually, it will be much easier to just subclass `HubbardSquareLattice` and define our own `site_pair_iter()` that allows us to specify 'even' or 'odd'. I'll call this class `DecomposedHubbardSquareLattice`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "class DecomposedHubbardSquareLattice(HubbardSquareLattice):\n",
    "    @property \n",
    "    def edge_types(self):\n",
    "        # Overriding edge_types property so we can define additional ones \n",
    "        return ('onsite', 'neighbor', 'diagonal_neighbor', 'horizontal_neighbor', 'vertical_neighbor', \n",
    "                'hor_even_neighbor', 'hor_odd_neighbor', 'ver_even_neighbor', 'ver_odd_neighbor')\n",
    "        \n",
    "    def site_pairs_iter(self, edge_type, ordered=True):\n",
    "        # `ordered` parameter just flips the order: if True, for each a,b -> (a,b), (b,a); if False, for each \n",
    "        # a,b -> (a,b), so we only get it once and it doesn't flip order. \n",
    "        # We WANT ordered=False, because there's a helper function tunneling_operator(i, j, coeff) that takes in \n",
    "        # two sites i,j and does coeff*(i^j j^i). \n",
    "        \n",
    "        # Overriding site_pairs_iter() to add functionality for additional edge_types \n",
    "        if edge_type == 'onsite':\n",
    "            return ((i, i) for i in self.site_indices)\n",
    "        elif edge_type == 'neighbor':\n",
    "            return self.neighbors_iter(ordered)\n",
    "        elif edge_type == 'horizontal_neighbor':\n",
    "            return self.horizontal_neighbors_iter(ordered)\n",
    "        elif edge_type == 'vertical_neighbor':\n",
    "            return self.vertical_neighbors_iter(ordered)\n",
    "        elif edge_type == 'diagonal_neighbor':\n",
    "            return self.diagonal_neighbors_iter(ordered)\n",
    "        # Above was copied from utils._lattice.py so we handle old edge_types correctly. \n",
    "        # Below is added functionality for new edge_types. \n",
    "        elif edge_type == 'hor_even_neighbor':\n",
    "            return self.hv_eo_neighbors(lambda x,y: 1-x%2, lambda x,y: (x+1, y), ordered) \n",
    "        elif edge_type == 'hor_odd_neighbor':\n",
    "            return self.hv_eo_neighbors(lambda x,y: x%2, lambda x,y: (x+1, y), ordered)\n",
    "        elif edge_type == 'ver_even_neighbor':\n",
    "            return self.hv_eo_neighbors(lambda x,y: 1-y%2, lambda x,y: (x, y+1), ordered)\n",
    "        elif edge_type == 'ver_odd_neighbor':\n",
    "            return self.hv_eo_neighbors(lambda x,y: y%2, lambda x,y: (x, y+1), ordered)\n",
    "        raise ValueError('Edge type {} is not valid.'.format(edge_type))\n",
    "        \n",
    "    def hv_eo_neighbors(self, filter_xy, map_next_xy, ordered=True):\n",
    "        for i in range(self.x_dimension):\n",
    "            for j in range(self.y_dimension):\n",
    "                if filter_xy(i, j):\n",
    "                    # Get indices for next site  \n",
    "                    k, l = map_next_xy(i, j)\n",
    "                    # Make sure next site isn't out of bounds \n",
    "                    if k >= self.x_dimension or l >= self.y_dimension: continue \n",
    "                    \n",
    "                    site_a = self.to_site_index((i, j))\n",
    "                    site_b = self.to_site_index((k, l))\n",
    "                    \n",
    "                    yield (site_a, site_b)\n",
    "                    if ordered: yield (site_b, site_a)\n",
    "        \n",
    "    # Function OpenFermion uses for 'horizontal_neightbor' edge_type.  \n",
    "    # PROBLEM: This loops around! Notice the % self.x_dimension. \n",
    "    def horizontal_neighbors_iter(self, ordered=True):\n",
    "        n_horizontal_edges_per_y = (\n",
    "            self.x_dimension - (self.x_dimension <= 2 or not self.periodic))\n",
    "        for x in range(n_horizontal_edges_per_y):\n",
    "            for y in range(self.y_dimension):\n",
    "                i = self.to_site_index((x, y))\n",
    "                j = self.to_site_index(((x + 1) % self.x_dimension, y))\n",
    "                yield (i, j)\n",
    "                if ordered:\n",
    "                    yield (j, i)\n",
    "\n",
    "    # Function OpenFermion uses for 'vertical_neighbor' edge_type\n",
    "    def vertical_neighbors_iter(self, ordered=True):\n",
    "        n_vertical_edges_per_x = (self.y_dimension -\n",
    "                                  (self.y_dimension <= 2 or not self.periodic))\n",
    "        for y in range(n_vertical_edges_per_x):\n",
    "            for x in range(self.x_dimension):\n",
    "                i = self.to_site_index((x, y))\n",
    "                j = self.to_site_index((x, (y + 1) % self.y_dimension))\n",
    "                yield (i, j)\n",
    "                if ordered:\n",
    "                    yield (j, i)\n",
    "    \n",
    "    # I'm changing this function so that it uses my functions, so I can compare the spectrums, since \n",
    "    # FermiHubbardModel.hamiltonian() will call this to generate the iterable. \n",
    "    def neighbors_iter(self, ordered=True):\n",
    "        return itertools.chain(\n",
    "            #self.horizontal_neighbors_iter(ordered),\n",
    "            #self.vertical_neighbors_iter(ordered)\n",
    "            \n",
    "            # itertools.chain('ABC', 'DEF') = Iterable('ABCDEF') \n",
    "            self.site_pairs_iter('hor_even_neighbor', ordered), \n",
    "            self.site_pairs_iter('hor_odd_neighbor', ordered), \n",
    "            self.site_pairs_iter('ver_even_neighbor', ordered), \n",
    "            self.site_pairs_iter('ver_odd_neighbor', ordered), \n",
    "        )\n",
    "\n",
    "\n",
    "lattice = DecomposedHubbardSquareLattice(x_n, y_n, n_dofs=n_dofs, periodic=periodic, spinless=spinless)\n",
    "hubbard = FermiHubbardModel(lattice , tunneling_parameters=tunneling, interaction_parameters=interaction, \n",
    "                            potential_parameters=potential, magnetic_field=mag_field, \n",
    "                            particle_hole_symmetry=particle_hole_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfermion.ops import FermionOperator\n",
    "\n",
    "def tunneling_operator(i, j, coefficient=1.):\n",
    "    # Copied from hamiltonians/_lattice.py in OpenFermion\n",
    "    return (FermionOperator(((i, 1), (j, 0)), coefficient) + FermionOperator(\n",
    "        ((j, 1), (i, 0)), coefficient.conjugate()))\n",
    "def tunneling_terms_hor_even(hor, even, model):\n",
    "    # Mostly copied from FermiHubbardMode.tunneling_terms() \n",
    "    terms = FermionOperator()\n",
    "    for param in model.tunneling_parameters:\n",
    "        a, aa = param.dofs \n",
    "        # We don't use param.edge_type because it's 'neighbor' and we need to be more specific\n",
    "        if hor and even:\n",
    "            site_pairs = model.lattice.site_pairs_iter('hor_even_neighbor', a != aa)\n",
    "        elif hor and not even: \n",
    "            site_pairs = model.lattice.site_pairs_iter('hor_odd_neighbor', a != aa)\n",
    "        elif not hor and even: \n",
    "            site_pairs = model.lattice.site_pairs_iter('ver_even_neighbor', a != aa)\n",
    "        elif not hor and not even:\n",
    "            site_pairs = model.lattice.site_pairs_iter('ver_odd_neighbor', a != aa)\n",
    "\n",
    "        for r, rr in site_pairs: \n",
    "            for spin_index in model.lattice.spin_indices:\n",
    "                i = model.lattice.to_spin_orbital_index(r, a, spin_index)\n",
    "                j = model.lattice.to_spin_orbital_index(rr, aa, spin_index)\n",
    "                terms += tunneling_operator(i, j, -param.coefficient)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamiltonians = {\n",
    "    'hub': hubbard.hamiltonian(), \n",
    "    'non_interacting': hubbard.tunneling_terms(), \n",
    "    'hor_even': tunneling_terms_hor_even(True, True, hubbard), \n",
    "    'hor_odd': tunneling_terms_hor_even(True, False, hubbard), \n",
    "    'ver_even': tunneling_terms_hor_even(False, True, hubbard), \n",
    "    'ver_odd': tunneling_terms_hor_even(False, False, hubbard)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure summing up the parts gives us the whole of non_interacting term \n",
    "print('Sum of horizontal/vertical even/odd terms gives non-interacting term: ', \n",
    "      (hamiltonians['hor_even'] + hamiltonians['hor_odd'] + hamiltonians['ver_even'] + \n",
    "       hamiltonians['ver_odd'] == hamiltonians['non_interacting']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing ground states of tunneling term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall we calculated w_tun and v_tun as the 16 degenerate lowest eigenvalues and eigenvectors of the \n",
    "# tunneling term\n",
    "v_tun.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_tun[:,0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "def decompose_HF(state):\n",
    "    \"\"\"Decomposes an eigenstate (I can generalize to any state later) as a tensor product of |0> and |1> \"\"\"\n",
    "    tot = 0\n",
    "    for i in range(len(state)):\n",
    "        if np.abs(state[i]) > tol:\n",
    "            tot += 1\n",
    "    return tot\n",
    "        \n",
    "\n",
    "\n",
    "# Why do they all have 196 nonzero coefficients?\n",
    "for j in range(16):\n",
    "    print(decompose_HF(v_tun[:,j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, right, I guess the ground state *doesn't have to be* a pure state in the computational basis. It has to first and foremost be an eigenvector. For the Hadamard matrix, the eigenvectors are $\\ket{+}$ and $\\ket{-}$, where $\\ket{-}$ is the ground state, for example. \n",
    "\n",
    "Okay, so I need to move this to a basis where the Hamiltonian is diagonal. That's just the Bogoliubov transform. Then each eigenvector should be a separate basis state. Before I do that, let's just verify that the 16 degenerate eigenvectors are orthogonal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_orthogonal(m):\n",
    "    \"\"\"Checks that each vector m[:,i] is orthogonal to the other vectors in m\"\"\"\n",
    "    for i in range(m.shape[1]):\n",
    "        vec1 = m[:,i]\n",
    "        for j in range(i+1, m.shape[1]):\n",
    "            vec2 = m[:,j]\n",
    "            overlap = fidelity(vec1, vec2)\n",
    "            assert np.abs(overlap) < tol, \"Inner product was {} for vectors {} and {}.\".format(overlap, i, j)\n",
    "            \n",
    "check_orthogonal(v_tun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, the eigenvectors of a Hermitian matrix are orthogonal if the eigenvalues are distinct. All these vectors have the same eigenvalues, so the eigenvectors don't have to be orthogonal. \n",
    "\n",
    "Well, we could make them orthogonal with Gram-Schmidt, but wouldn't that change the eigenvalues? No, consider\n",
    "$$ H ( \\ket{\\psi_0} + \\ket{\\psi_1} ) = E_1 \\ket{\\psi_0} + ? \\ket{\\psi_1} $$\n",
    "where $\\ket{\\psi_0}$ is the first eigenvector, and we decomposed the second eigenvector as $\\ket{\\psi_0} + \\ket{\\psi_1}$. Then, by definition of eigenvector, the eigenvalue of $\\ket{\\psi_1}$ must be $E_1$. So, Gram-Schmidt preserves eigenvalues. \n",
    "\n",
    "Simple enough, I'll do GS on the degenerate ground states. Fine, but then what? After the Bogoliubov transform, the eigenvectors no longer represent distinct pure states in HF space. So how can I analyze this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubbard ground state is a superposition of 16 HF states\n",
    "decompose_HF(v_hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2 x 1 lattice, `v_hub` is a superposition of 4 states, and for a 2 x 2 lattice, `v_hub` is a superposition for 16 states. I'm guessing it's a function $2^{\\mathrm{\\# sites}}$, but why? \n",
    "\n",
    "Hypothesis 1: up and down spins are symmetric here since we don't really have different behavior for them. So, suppose the ground state is defined for $x$ fermions where $x = N / 2$, ie half-filling. The $2^x$ occurs because we can choose to put each fermion in either UP spin or DOWN spin, without changing the energy of the system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
